{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.11.4\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n",
    "#!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading text data\n",
    "\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  1115394\n"
     ]
    }
   ],
   "source": [
    "print(\"length of dataset in characters: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# let's look at the first 1000 characters\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "# here are all the unique characters that occur in this text\n",
    "# these are the possible characters the model can see or emit\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 1, 58, 46, 43, 56, 43]\n",
      "hi there\n"
     ]
    }
   ],
   "source": [
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars)} # creating a lookup table char to integer\n",
    "itos = { i:ch for i,ch in enumerate(chars)} # creating a lookup table ineger to char\n",
    "encode = lambda s: [stoi[c] for c in s ] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of intergers, output a string\n",
    "\n",
    "'''\n",
    "in our experiment we are using character level tokenizer.\n",
    "we only have 65 vocab. so, our integer sequence length will be big/long\n",
    "'''\n",
    "\n",
    "print(encode('hi there'))\n",
    "print(decode(encode('hi there')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's now encode the entire text of dataset and store it into a torch.Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
      "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
      "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
      "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
      "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
      "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
      "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
      "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
      "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
      "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
      "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
      "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
      "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
      "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
      "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
      "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
      "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
      "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
      "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
      "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
      "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
      "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
      "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
      "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
      "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
      "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
      "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
      "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
      "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gn/2x1vvpwj6qzbnf9ntmrq2h5r0000gn/T/ipykernel_4937/699030071.py:3: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  data = torch.tensor(encode(text), dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000]) # the 1000 characters we looked at earlier will to the GPT look like this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split up the data into train and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n    so, this will help us understand to what extent our model is overfitting.\\n    we don't want exact memorization of the exact shakespeare.\\n    we want nn that creates shakespeare like text\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = int(0.9*len(data))\n",
    "\n",
    "train_data = data[:n] # first 90% will be train and rest validation data\n",
    "val_data = data[n:]\n",
    "\n",
    "'''\n",
    "    so, this will help us understand to what extent our model is overfitting.\n",
    "    we don't want exact memorization of the exact shakespeare.\n",
    "    we want nn that creates shakespeare like text\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Dimension of the tensors that are going to be feeding into the transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    Training a transformer on entire dataset is computationally expensive.\n",
    "    So, we will train the model based on chunks/batches (random sample of the datasets).\n",
    "    These chunks have some maximum length.\n",
    "    there are some other names context_length or block size.\n",
    "    Here, we will call as a block size\n",
    "'''\n",
    "\n",
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### the above output means:\n",
    "                - In the context of 18, 47 likely comes next\n",
    "                - In the context of 18, 47, the integer 56 will come next\n",
    "                like wise ....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]) the target: 47\n",
      "when input is tensor([18, 47]) the target: 56\n",
      "when input is tensor([18, 47, 56]) the target: 57\n",
      "when input is tensor([18, 47, 56, 57]) the target: 58\n",
      "when input is tensor([18, 47, 56, 57, 58]) the target: 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size] # input to the transformers, just the block size of characters\n",
    "y = train_data[1:block_size+1] # will be the next block size characters. so, basically it means offset by one.\n",
    "                                # because y are the targets for each position in the input\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} the target: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "\n",
    "    - above are the 8 examples hidden in the chunk of nine characters that we sampled from the training set\n",
    "    chunk - tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])\n",
    "    - we train all the examples with context between one all the way up to context of block size\n",
    "    - we did this way not only because of computationally expensive. But,\n",
    "    - we want the Transformer network be used to seeing contexts all the way from as little as one all the way to block size\n",
    "    - And we'd like the tranformer to be used to seeing everthing in between\n",
    "        - that's going to be useful later during inference.\n",
    "        - Because, while we're sampling we can start the sampling generation with as little as one character of context\n",
    "        - And, the tranformers know how to predict the next character with all the way up to just one context one and so\n",
    "        - then it can predict everything up to block size and after block size we have to start truncating because the Transformers\n",
    "          will never receive more than the block size inputs when it's predicting the next character"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "----------\n",
      "when the input is [24] the target: 43\n",
      "when the input is [24, 43] the target: 58\n",
      "when the input is [24, 43, 58] the target: 5\n",
      "when the input is [24, 43, 58, 5] the target: 57\n",
      "when the input is [24, 43, 58, 5, 57] the target: 1\n",
      "when the input is [24, 43, 58, 5, 57, 1] the target: 46\n",
      "when the input is [24, 43, 58, 5, 57, 1, 46] the target: 43\n",
      "when the input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39\n",
      "when the input is [44] the target: 53\n",
      "when the input is [44, 53] the target: 56\n",
      "when the input is [44, 53, 56] the target: 1\n",
      "when the input is [44, 53, 56, 1] the target: 58\n",
      "when the input is [44, 53, 56, 1, 58] the target: 46\n",
      "when the input is [44, 53, 56, 1, 58, 46] the target: 39\n",
      "when the input is [44, 53, 56, 1, 58, 46, 39] the target: 58\n",
      "when the input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1\n",
      "when the input is [52] the target: 58\n",
      "when the input is [52, 58] the target: 1\n",
      "when the input is [52, 58, 1] the target: 58\n",
      "when the input is [52, 58, 1, 58] the target: 46\n",
      "when the input is [52, 58, 1, 58, 46] the target: 39\n",
      "when the input is [52, 58, 1, 58, 46, 39] the target: 58\n",
      "when the input is [52, 58, 1, 58, 46, 39, 58] the target: 1\n",
      "when the input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46\n",
      "when the input is [25] the target: 17\n",
      "when the input is [25, 17] the target: 27\n",
      "when the input is [25, 17, 27] the target: 10\n",
      "when the input is [25, 17, 27, 10] the target: 0\n",
      "when the input is [25, 17, 27, 10, 0] the target: 21\n",
      "when the input is [25, 17, 27, 10, 0, 21] the target: 1\n",
      "when the input is [25, 17, 27, 10, 0, 21, 1] the target: 54\n",
      "when the input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337) # sampling random locations in the dataset to pull chunks from, here we are setting the seed in the random number generator to get determinstic output all the time\n",
    "batch_size = 4 # how many independent sequences will be process (in every forward and backward pass) in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions\n",
    "\n",
    "def get_batch(split):\n",
    "    '''generate a small batch of data of inputs x and targets y\n",
    "    '''\n",
    "    \n",
    "    data = train_data if split == 'train' else val_data # if split equal to train will look into train or else val_data\n",
    "    '''\n",
    "    ix descriptions:\n",
    "        when I generate random positions to grab a chuk out of data.\n",
    "        I actually, generate batch size number of random offsets.\n",
    "        ex: so, batch_size = 4, so ix going to be a 4 numbers that are randomly generated between 0 and len(data)-block_size\n",
    "    '''\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size, )) # it's just the random offsets in the training set\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix]) # first block size characters starting at i the y's are the offset by one of that\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "\n",
    "    return x,y\n",
    "\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape) # 4x8 matrix, 4 batche_size, 8 - each block size\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "\n",
    "print('----------')\n",
    "\n",
    "for b in range(batch_size): # batch dimension\n",
    "    for t in range(block_size): # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f'when the input is {context.tolist()} the target: {target}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "    - this 4x8 array contains a completely independent as far as the Transformer is concerned\n",
    "    - These are 32 independet examples packed into a single batch of the input x and the desired targets y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n"
     ]
    }
   ],
   "source": [
    "print(xb) # our input to the tranformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BigramLanguageModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 65])\n",
      "tensor(4.7108, grad_fn=<NllLossBackward0>)\n",
      "The generation we achieved\n",
      "idx = tensor([[0]])\n",
      "\n",
      "SKIcLT;AcELMoTbvZv C?nq-QE33:CJqkOKH-q;:la!oiywkHjgChzbQ?u!3bLIgwevmyFJGUGp\n",
      "wnYWmnxKWWev-tDqXErVKLgJ\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size) -> None:\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size) # nn.Embedding is a very thin wrapper, basically a tensor of shape vocab_size x vocab_size \n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        #idx and targets are both (B,T) tensor of integers\n",
    "        ''' \n",
    "        idx: tensor of integers\n",
    "        targets: target tensor of integers\n",
    "\n",
    "        self.token_embedding_table(idx): when we pass index ([[24, 43, 58,  5, 57,  1, 46, 43]]) here,\n",
    "                                         every single integer in our index is going to refer this embedding table and pluck out the row of that embedding table corresponding to that index.\n",
    "                                         eg: index 24 will go into the embedding table and pluck out the 24th row,\n",
    "                                         this 24th row has a scores for the next character in the sequence.\n",
    "        logits: (B,T,C), 4 x 8 x 65\n",
    "        are basically the scores for the next character in the sequence\n",
    "        '''\n",
    "        \n",
    "        logits = self.token_embedding_table(idx) # (B,T,C), B - Batch (4), T - Time (8), C - Channel (vocab_size) (65)\n",
    "        # thru logits we've made predictions about what comes next\n",
    "        # and now we'd like to evaluate the loss functions\n",
    "        # a good way to measure a loss or quality of predictions is to use the negative log likelihood loss in pytorch it is referred as cross_entropy\n",
    "\n",
    "        # predicted (logits) and true values (targets)\n",
    "\n",
    "        # usually cross_entropy from pytorch expects the input in (B,C,T) But we have B,T,C\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets) \n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        ''' This function will take idx (B, T) array of indices in the current context\n",
    "            and generate B by T+1, T+2 or T+3 as many as we want maximum tokens.\n",
    "            So, this is generation from the model.\n",
    "\n",
    "            This generate will operate based on batches.\n",
    "\n",
    "        Arguments:\n",
    "            idx: This is the current context of some characters in a batch/some batch\n",
    "            max_new_tokens: max no.of T+1 characters\n",
    "        '''\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            # here we are not using the loss, as we don't have any ground truth targets, it's getting ignored\n",
    "            logits, loss = self(idx)\n",
    "            #print(f'logits before tranformations = {logits} and shape = {logits.shape}')\n",
    "\n",
    "            # focus only on the last time step, we are plucking out the last char/digit in the time dimension(-1) because that last char/digit is the prediction\n",
    "            logits = logits[:, -1, :] # becomes (B,C)\n",
    "            #print(f'logits after tranformations = {logits} and shape = {logits.shape}')\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B,C)\n",
    "            #print(f'probs = {probs}')\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            #print(f'idx_next = {idx_next}')\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "#tst_x = torch.tensor([[24, 43, 58,  5, 57,  1, 46, 43]])\n",
    "#tst_y = torch.tensor([[43, 58,  5, 57,  1, 46, 43, 39]])\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "# here 1x1 tensor which is of zeros, in our encoding zero represents new line\n",
    "print('The generation we achieved')\n",
    "idx = torch.zeros((1,1), dtype=torch.long)\n",
    "print(f'idx = {idx}')\n",
    "print(decode(m.generate(idx, max_new_tokens=100)[0].tolist()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "\n",
    "    - when we do -ln(1/65) = 4.174. But, we got the loss as 4.8786\n",
    "    - So, our inital predictions are not super diffuse, we got little bit of entropy. so, we're guessing wrong\n",
    "    - But, now we are evaluate the loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a PyTorch optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.684566974639893\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "for steps in range(10000):\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    # zeroing out all the gradients from previous step and getting\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    # getting the gradients for all the parameters\n",
    "    loss.backward()\n",
    "    # and using those gradients to update their parameters\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "pm foglO:p?Gbz'jbO:CKHxmhsEWe,rEq f jnQw'YGnYW:VJiKu CKb$vZbjgjQMV&qVTQlgnENs$.?O U'yjyff?h\n",
      "'zxZnWtKpgu3KpoiPFhix\n",
      "SJ'\n",
      "yFjjN3Q&u3WgMmywW$GJL?sPY?YtNpErgIo,XcJ&DniqVmZBwfFD3faBoi'I3Q?$xBr&G,yxUN!Wsyy cLC-IBothTsze'W?q;!:xrFti.ZQyJyQ!u!zHK$EQq-wM.T'QUiN.SPyjKtL&vbRwW,SZBCj?aUIUxDALpAUGFbLQXNVY.sBId-'wNm;UL:3Sq-KAiqRiN:lL-Qm,iCulyZUAP,oSB3SlriFWiy;GJ,NuUxxTER-!a!UYN\n",
      "kOpJU'usc UIo.JAa!U CjuRw'TthF;aLq-KX&jzCE;HzE-Wla!uDKBjuxVBk!UQXlFlaXt,wWWV&G;HU?zW:.ObX?wfRkRyJBUzK$Evi\n",
      "hjdlgGrT?I33StKa;AUGJAE-WWgI'\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx = torch.zeros((1,1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mathematical trick in self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# toy example\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "'''Here, we've up to 8 tokens in a batch.\n",
    "   currently, these 8 tokens are not talking to each other.\n",
    "   we would like to talk to each other/ couple them each other\n",
    "   and in particular, the token at 5th location should not communicate at token at 6th, 7th ...\n",
    "   because those are the future tokens in a sequence.\n",
    "   so, the token on the 5th location should only talk to the one in the 4th, 3rd, 2nd, 1st\n",
    "   Information only flows from previous context to the current timestep\n",
    "'''\n",
    "B,T,C = 4,8,2 # batch, time, channel\n",
    "\n",
    "x = torch.randn(B, T, C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Now, if I want my tokens to talk each other the simplest way is to \\n   average the all the previous channels, information at my step but then also the\\n   channels from the four step, third, 2 and 1.\\n   I'd like to average those up and then it becomes the feature vector that summarizes me in the context of my history.\\n   weak form of interaction: an average is extremely weak form of interaction, we've lost the ton of information on spatial arrangements of\\n                             all those tokens but that's ok for now\\n\""
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Now, if I want my tokens to talk each other the simplest way is to \n",
    "   average the all the previous channels, information at my step but then also the\n",
    "   channels from the four step, third, 2 and 1.\n",
    "   I'd like to average those up and then it becomes the feature vector that summarizes me in the context of my history.\n",
    "   weak form of interaction: an average is extremely weak form of interaction, we've lost the ton of information on spatial arrangements of\n",
    "                             all those tokens but that's ok for now\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want x[b,t] = mean_{i<=t} x[b,i]\n",
    "xbow = torch.zeros((B,T,C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b, :t+1] # (t,c)\n",
    "        xbow[b,t] = torch.mean(xprev, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.4 ('nano_gpt')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "352ccd192e5415136a0e720f92e561ce763695731882180d8e49dde77a5ed9d2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
